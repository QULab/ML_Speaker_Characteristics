{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating regression techniques for speaker characterization\n",
    "### Laura Fernández Gallardo\n",
    "\n",
    "Multioutput regression.\n",
    "\n",
    "Motifications with respect to regression with 1-dimensional output:\n",
    "\n",
    "Targets: 34-dimensional ratings of [speaker characteristics](https://github.com/laufergall/Subjective_Speaker_Characteristics) given to the 300 speakers of the [NSC](http://www.qu.tu-berlin.de/?id=nsc-corpus) corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from reg_tuning import * # my helper functions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 2302\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/laufergall/ML_Speaker_Characteristics/master/data/generated_data/'\n",
    "\n",
    "url = path + \"feats_ratings_scores_train.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_train = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "url = path + \"feats_ratings_scores_test.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_test = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "with open(r'..\\data\\generated_data\\feats_names.txt') as f:\n",
    "    feats_names = f.readlines()\n",
    "feats_names = [x.strip().strip('\\'') for x in feats_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\items_names.txt') as f:\n",
    "    items_names = f.readlines()\n",
    "items_names = [x.strip().strip('\\'') for x in items_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\traits_names.txt') as f:\n",
    "    traits_names = f.readlines()\n",
    "traits_names = [x.strip().strip('\\'') for x in traits_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34items: Model tuning\n",
    "\n",
    "Use the train data to find the regressor and its hyperparameters leading to the best performance. \n",
    "\n",
    "Not performing feature selection with \"SelectKBest\": Univariate feature selection does not support multilabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize speech features  \n",
    "\n",
    "dropcolumns = ['name','spkID','speaker_gender'] + items_names + traits_names\n",
    "\n",
    "# learn transformation on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats_ratings_scores_train.drop(dropcolumns, axis=1))\n",
    "\n",
    "# numpy n_instances x n_feats\n",
    "feats_s_train = scaler.transform(feats_ratings_scores_train.drop(dropcolumns, axis=1))\n",
    "feats_s_test = scaler.transform(feats_ratings_scores_test.drop(dropcolumns, axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data. Features and labels\n",
    "X = feats_s_train # (2700, 88)\n",
    "y = feats_ratings_scores_train[items_names].as_matrix() # (2700, 34)\n",
    "\n",
    "# test data. Features and labels\n",
    "Xt = feats_s_test # (891, 88)\n",
    "yt = feats_ratings_scores_test[items_names].as_matrix() # (891, 34)\n",
    "\n",
    "# split train data into 80% and 20% subsets - with balance in trait and gender\n",
    "# give subset A to the inner hyperparameter tuner\n",
    "# and hold out subset B for meta-evaluation\n",
    "AX, BX, Ay, By = train_test_split(X, y, \n",
    "                                  test_size=0.20, \n",
    "                                  stratify = feats_ratings_scores_train['speaker_gender'], \n",
    "                                  random_state=2302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe with results from hp tuner to be appended\n",
    "tuning_all = pd.DataFrame()\n",
    "\n",
    "# list with tuned models trained on training data, to be appended\n",
    "trained_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save splits\n",
    "\n",
    "label = 'multioutput_34items'\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "np.save(r'.\\data_while_tuning\\X_' + label + '.npy', X)\n",
    "np.save(r'.\\data_while_tuning\\y_' + label + '.npy', y)\n",
    "np.save(r'.\\data_while_tuning\\Xt_' + label + '.npy', Xt)\n",
    "np.save(r'.\\data_while_tuning\\yt_' + label + '.npy', yt)\n",
    "\n",
    "# # A/B splits, features and labels\n",
    "np.save(r'.\\data_while_tuning\\AX_' + label + '.npy', AX)\n",
    "np.save(r'.\\data_while_tuning\\BX_' + label + '.npy', BX)\n",
    "np.save(r'.\\data_while_tuning\\Ay_' + label + '.npy', Ay)\n",
    "np.save(r'.\\data_while_tuning\\By_' + label + '.npy', By)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling hp_tuner() for each regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Recover ** when new ipynb session started.\n",
    "\n",
    "(Workaround for working with hyperparameter tuning during several days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'multioutput_34items'\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "X = np.load(r'.\\data_while_tuning\\X_' + label + '.npy')\n",
    "y = np.load(r'.\\data_while_tuning\\y_' + label + '.npy')\n",
    "Xt = np.load(r'.\\data_while_tuning\\Xt_' + label + '.npy')\n",
    "yt = np.load(r'.\\data_while_tuning\\yt_' + label + '.npy')\n",
    "\n",
    "# A/B splits, features and labels\n",
    "AX = np.load(r'.\\data_while_tuning\\AX_' + label + '.npy')\n",
    "BX = np.load(r'.\\data_while_tuning\\BX_' + label + '.npy')\n",
    "Ay = np.load(r'.\\data_while_tuning\\Ay_' + label + '.npy')\n",
    "By = np.load(r'.\\data_while_tuning\\By_' + label + '.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Loading outpus of hp tuning from disk\n",
    "# tuning_all, trained_all = load_tuning(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this after each experiment **to recover later**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # save tuning_all (.csv) and trained_all (namemodel.sav)\n",
    "# save_tuning(tuning_all, trained_all, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "*class sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)*\n",
    "\n",
    "Tune: max_features, max_depth, min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'RandomForestRegressor' -> Best cross-val score on A set: -49.782665 using {'regressor__estimator__min_samples_leaf': 2, 'regressor__estimator__max_features': 40, 'regressor__estimator__max_depth': 22}\n",
      "'RandomForestRegressor' -> root mean_squared_error on B set: 6.850574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "def get_RandomForestRegressor2tune():\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=2302, max_features = None)\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_features = np.arange(2,50),\n",
    "        regressor__estimator__max_depth = np.arange(2,50), \n",
    "        regressor__estimator__min_samples_leaf = np.arange(2,50) \n",
    "    )\n",
    "    return 'RandomForestRegressor', model, hp\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_RandomForestRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='random',\n",
    "                           n_iter=50\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'RandomForestRegressor' -> Best cross-val score on A set: -49.731643 using {'regressor__estimator__min_samples_leaf': 3, 'regressor__estimator__max_features': 40, 'regressor__estimator__max_depth': 34}\n",
      "'RandomForestRegressor' -> root mean_squared_error on B set: 6.886105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "def get_RandomForestRegressor2tune():\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=2302, max_features = None)\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_features = np.arange(30,50),\n",
    "        regressor__estimator__max_depth = np.arange(15,35), \n",
    "        regressor__estimator__min_samples_leaf = np.arange(2,5) \n",
    "    )\n",
    "    return 'RandomForestRegressor', model, hp\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_RandomForestRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='random',\n",
    "                           n_iter=50\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update lists of tuning info and trained regressors\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)\n",
    "\n",
    "# save tuning_all (.csv) and trained_all (nameregressor.sav)\n",
    "save_tuning(tuning_all, trained_all, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (Error) Multilayer Perceptron for regression\n",
    "\n",
    "TODO: Solve error: related to the shape of the target vector.\n",
    "\n",
    "Defining model architecture: Multilayer Perceptron (MLP) using keras with TensorFlow backend.\n",
    "\n",
    "* input the (standardized) speech features (input_dim = 88)\n",
    "* one fully-connected hidden layer with the same number of neurons as input speech features\n",
    "* output with linear activation function (for regession) with the same number of neurons as output speaker characteristics\n",
    "\n",
    "Tuning hyperparameters using StratifiedKFold cross-validation with set A:\n",
    "\n",
    "* number of epochs\n",
    "* batch size\n",
    "* number of hidden units\n",
    "* optimizer\n",
    "* learning rate\n",
    "* activation function hidden layer\n",
    "* dropout regularization\n",
    "\n",
    "Evaluating performance on the hold-out set B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\"\"\"\n",
    "MLP with KerasRegressor\n",
    "\"\"\"\n",
    "\n",
    "def create_model(optimizer = 'Adam', learn_rate=0.2, neurons=1, activation='relu', dropout_rate=0.0):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons,\n",
    "                    activation=activation, \n",
    "                    input_dim=88))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(34))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laura\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:115: DeprecationWarning: Estimator KerasRegressor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (34,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-cff6bc300c6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                            \u001b[1;33m[\u001b[0m\u001b[1;36m88\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# no feature selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                            \u001b[1;34m'random'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                            \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                           )\n",
      "\u001b[1;32mD:\\Work\\git_ML\\ML_Speaker_Characteristics\\regression\\reg_tuning.py\u001b[0m in \u001b[0;36mhp_tuner\u001b[1;34m(AX, BX, Ay, By, get_reg_functions, label, feats_names, k_array, mode, n_iter)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m# This might take a while:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0msearch_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m# summary of hp tuning on set A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    168\u001b[0m             delayed(_fit_estimator)(\n\u001b[0;32m    169\u001b[0m                 self.estimator, X, y[:, i], sample_weight)\n\u001b[1;32m--> 170\u001b[1;33m             for i in range(y.shape[1]))\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1591\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1593\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1594\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1431\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1432\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    118\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (34,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# 1st round (tuning epochs, batch_size, neurons, learn rate):\n",
    "    \n",
    "def get_KerasRegressor2tune():\n",
    "    \n",
    "    model = KerasRegressor(build_fn = create_model, verbose=0)\n",
    "                        \n",
    "    hp = dict(\n",
    "        regressor__estimator__epochs = [25,50,75,100],\n",
    "        regressor__estimator__batch_size = [5,10], \n",
    "        regressor__estimator__neurons = [40, 60, 80, 160],\n",
    "        regressor__estimator__learn_rate = np.arange(start=0.2, stop=1.0, step=0.05) \n",
    "        #regressor__estimator__activation = ['relu'], # ['softmax', 'softplus', 'sofsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "        #regressor__estimator__dropout_rate = [0.5], # np.arange(start=0, stop=1, step=0.1)\n",
    "        #regressor__estimator__optimizer = ['Adam'] #['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    )\n",
    "    return 'KerasRegressor', model, hp\n",
    "\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_KerasRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # no feature selection\n",
    "                           'random',\n",
    "                           n_iter=20\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 2nd round (tuning activation and dropout_rate):\n",
    "    \n",
    "# def get_KerasRegressor2tune():\n",
    "    \n",
    "#     model = KerasRegressor(build_fn = create_model, verbose=0)\n",
    "                        \n",
    "#     hp = dict(\n",
    "#         regressor__estimator__epochs = [75], #[25,50,75,100],\n",
    "#         regressor__estimator__batch_size = [5], # [5,10], \n",
    "#         regressor__estimator__neurons = [160], #[40, 80, 160],\n",
    "#         regressor__estimator__learn_rate = [0.8], # np.arange(start=0.2, stop=1.0, step=0.05) \n",
    "#         regressor__estimator__activation = ['relu','tanh','sigmoid','linear'], # ['softmax', 'softplus', 'sofsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "#         regressor__estimator__dropout_rate = np.arange(start=0, stop=1, step=0.2)\n",
    "#         #regressor__estimator__optimizer = ['Adam'] #['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#     )\n",
    "#     return 'KerasRegressor', model, hp\n",
    "\n",
    "\n",
    "# tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "#                            [get_KerasRegressor2tune], \n",
    "#                            label,\n",
    "#                            feats_names,\n",
    "#                            [88], # no feature selection\n",
    "#                            'grid'\n",
    "#                           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree\n",
    "\n",
    "*class sklearn.tree.DecisionTreeRegressor(criterion=’mse’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False)*\n",
    "\n",
    "Tune: max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DecisionTreeRegressor' -> Best cross-val score on A set: -70.723342 using {'regressor__estimator__max_features': 42, 'regressor__estimator__max_depth': 5}\n",
      "'DecisionTreeRegressor' -> root mean_squared_error on B set: 8.227232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\"\"\"\n",
    "Decision Trees\n",
    "\"\"\"\n",
    "def get_DecisionTreeRegressor2tune():\n",
    "    \n",
    "    model = DecisionTreeRegressor()\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_depth = np.arange(2,20), \n",
    "        regressor__estimator__max_features = np.arange(2,50)\n",
    "    )\n",
    "    return 'DecisionTreeRegressor', model, hp\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_DecisionTreeRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='random',\n",
    "                           n_iter=30\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update lists of tuning info and trained regressors\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)\n",
    "\n",
    "# save tuning_all (.csv) and trained_all (nameregressor.sav)\n",
    "save_tuning(tuning_all, trained_all, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "model = DummyRegressor(strategy='mean')\n",
    "model.fit(AX, Ay)\n",
    "By_pred = model.predict(BX)\n",
    "score_on_B = np.sqrt(mean_squared_error(By, By_pred))\n",
    "d = {\n",
    "    'regressors_names': ['DummyRegressor'],\n",
    "    'best_accs': score_on_B,\n",
    "    'best_hps': '',\n",
    "    'sel_feats': '',\n",
    "    'sel_feats_i': ''\n",
    "    }\n",
    "\n",
    "tuning = pd.DataFrame(data = d)\n",
    "trained = model.fit(X, y)\n",
    "\n",
    "# update lists of tuning info and trained regressors\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append([trained])\n",
    "\n",
    "# save tuning_all (.csv) and trained_all (nameregressor.sav)\n",
    "save_tuning(tuning_all, trained_all, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### RMSE\n",
    "Testing the best model found after hyperparameter tuning.\n",
    "\n",
    "Performance metric: RMSE.\n",
    "    \n",
    "The predictions correspond to the scores of each test segment (3 parts x 4 dialogs) spoken by the same test speaker. I perform the average of the predicted scores that correspond to the same speaker - to be compared to the true scores.\n",
    "\n",
    "Visualization: pairplot of scores of true and predicted test data, grouped-averaged by speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data if new ipynb session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_accs</th>\n",
       "      <th>best_hps</th>\n",
       "      <th>regressors_names</th>\n",
       "      <th>sel_feats</th>\n",
       "      <th>sel_feats_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.886105</td>\n",
       "      <td>{'regressor__estimator__min_samples_leaf': 3, ...</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>['F0semitoneFrom27.5Hz_sma3nz_amean', 'F0semit...</td>\n",
       "      <td>[ True  True  True  True  True  True  True  Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.886105</td>\n",
       "      <td>{'regressor__estimator__min_samples_leaf': 3, ...</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>['F0semitoneFrom27.5Hz_sma3nz_amean', 'F0semit...</td>\n",
       "      <td>[ True  True  True  True  True  True  True  Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.227232</td>\n",
       "      <td>{'regressor__estimator__max_features': 42, 're...</td>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>['F0semitoneFrom27.5Hz_sma3nz_amean', 'F0semit...</td>\n",
       "      <td>[ True  True  True  True  True  True  True  Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.356033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_accs                                           best_hps  \\\n",
       "0   6.886105  {'regressor__estimator__min_samples_leaf': 3, ...   \n",
       "1   6.886105  {'regressor__estimator__min_samples_leaf': 3, ...   \n",
       "2   8.227232  {'regressor__estimator__max_features': 42, 're...   \n",
       "3  10.356033                                                NaN   \n",
       "\n",
       "        regressors_names                                          sel_feats  \\\n",
       "0  RandomForestRegressor  ['F0semitoneFrom27.5Hz_sma3nz_amean', 'F0semit...   \n",
       "1  RandomForestRegressor  ['F0semitoneFrom27.5Hz_sma3nz_amean', 'F0semit...   \n",
       "2  DecisionTreeRegressor  ['F0semitoneFrom27.5Hz_sma3nz_amean', 'F0semit...   \n",
       "3         DummyRegressor                                                NaN   \n",
       "\n",
       "                                         sel_feats_i  \n",
       "0  [ True  True  True  True  True  True  True  Tr...  \n",
       "1  [ True  True  True  True  True  True  True  Tr...  \n",
       "2  [ True  True  True  True  True  True  True  Tr...  \n",
       "3                                                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 'multioutput_34items'\n",
    "\n",
    "\n",
    "path = 'https://raw.githubusercontent.com/laufergall/ML_Speaker_Characteristics/master/data/generated_data/'\n",
    "\n",
    "url = path + \"feats_ratings_scores_train.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_train = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "url = path + \"feats_ratings_scores_test.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_test = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "with open(r'..\\data\\generated_data\\feats_names.txt') as f:\n",
    "    feats_names = f.readlines()\n",
    "feats_names = [x.strip().strip('\\'') for x in feats_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\items_names.txt') as f:\n",
    "    items_names = f.readlines()\n",
    "items_names = [x.strip().strip('\\'') for x in items_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\traits_names.txt') as f:\n",
    "    traits_names = f.readlines()\n",
    "traits_names = [x.strip().strip('\\'') for x in traits_names] \n",
    "\n",
    "# train/test partitions, features and labels\n",
    "X = np.load(r'.\\data_while_tuning\\X_' + label + '.npy')\n",
    "y = np.load(r'.\\data_while_tuning\\y_' + label + '.npy')\n",
    "Xt = np.load(r'.\\data_while_tuning\\Xt_' + label + '.npy')\n",
    "yt = np.load(r'.\\data_while_tuning\\yt_' + label + '.npy')\n",
    "\n",
    "# Loading outpus of hp tuning from disk\n",
    "tuning_all, trained_all = load_tuning(label)\n",
    "tuning_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected classifier based on the best performance on B: 'RandomForestRegressor' (perf. on B = 6.89)\n"
     ]
    }
   ],
   "source": [
    "# select the classifier that gave the maximum acc on B set\n",
    "best_accs = tuning_all['best_accs']\n",
    "i_best = best_accs.idxmin()\n",
    "\n",
    "print('Selected classifier based on the best performance on B: %r (perf. on B = %0.2f)' % (tuning_all.loc[i_best,'regressors_names'], round(best_accs[i_best],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'RandomForestRegressor' -> avg RMSE 'non_likable' = 10.01\n",
      "'RandomForestRegressor' -> avg R2 'non_likable' = 0.12\n",
      "'RandomForestRegressor' -> avg RMSE 'secure' = 10.65\n",
      "'RandomForestRegressor' -> avg R2 'secure' = 0.11\n",
      "'RandomForestRegressor' -> avg RMSE 'attractive' = 9.12\n",
      "'RandomForestRegressor' -> avg R2 'attractive' = 0.17\n",
      "'RandomForestRegressor' -> avg RMSE 'unsympathetic' = 7.06\n",
      "'RandomForestRegressor' -> avg R2 'unsympathetic' = 0.17\n",
      "'RandomForestRegressor' -> avg RMSE 'indecisive' = 9.55\n",
      "'RandomForestRegressor' -> avg R2 'indecisive' = 0.04\n",
      "'RandomForestRegressor' -> avg RMSE 'unobtrusive' = 8.75\n",
      "'RandomForestRegressor' -> avg R2 'unobtrusive' = -0.09\n",
      "'RandomForestRegressor' -> avg RMSE 'distant' = 9.17\n",
      "'RandomForestRegressor' -> avg R2 'distant' = 0.17\n",
      "'RandomForestRegressor' -> avg RMSE 'bored' = 8.82\n",
      "'RandomForestRegressor' -> avg R2 'bored' = 0.37\n",
      "'RandomForestRegressor' -> avg RMSE 'emotional' = 8.10\n",
      "'RandomForestRegressor' -> avg R2 'emotional' = 0.36\n",
      "'RandomForestRegressor' -> avg RMSE 'not_irritated' = 9.70\n",
      "'RandomForestRegressor' -> avg R2 'not_irritated' = 0.03\n",
      "'RandomForestRegressor' -> avg RMSE 'active' = 8.57\n",
      "'RandomForestRegressor' -> avg R2 'active' = 0.35\n",
      "'RandomForestRegressor' -> avg RMSE 'pleasant' = 9.83\n",
      "'RandomForestRegressor' -> avg R2 'pleasant' = 0.06\n",
      "'RandomForestRegressor' -> avg RMSE 'characterless' = 7.26\n",
      "'RandomForestRegressor' -> avg R2 'characterless' = 0.27\n",
      "'RandomForestRegressor' -> avg RMSE 'sociable' = 8.96\n",
      "'RandomForestRegressor' -> avg R2 'sociable' = 0.22\n",
      "'RandomForestRegressor' -> avg RMSE 'relaxed' = 9.13\n",
      "'RandomForestRegressor' -> avg R2 'relaxed' = 0.08\n",
      "'RandomForestRegressor' -> avg RMSE 'affectionate' = 9.36\n",
      "'RandomForestRegressor' -> avg R2 'affectionate' = 0.21\n",
      "'RandomForestRegressor' -> avg RMSE 'dominant' = 9.06\n",
      "'RandomForestRegressor' -> avg R2 'dominant' = 0.08\n",
      "'RandomForestRegressor' -> avg RMSE 'unaffected' = 8.11\n",
      "'RandomForestRegressor' -> avg R2 'unaffected' = -0.01\n",
      "'RandomForestRegressor' -> avg RMSE 'hearty' = 9.52\n",
      "'RandomForestRegressor' -> avg R2 'hearty' = 0.22\n",
      "'RandomForestRegressor' -> avg RMSE 'old' = 11.94\n",
      "'RandomForestRegressor' -> avg R2 'old' = 0.12\n",
      "'RandomForestRegressor' -> avg RMSE 'personal' = 7.16\n",
      "'RandomForestRegressor' -> avg R2 'personal' = 0.04\n",
      "'RandomForestRegressor' -> avg RMSE 'calm' = 10.11\n",
      "'RandomForestRegressor' -> avg R2 'calm' = 0.10\n",
      "'RandomForestRegressor' -> avg RMSE 'incompetent' = 8.15\n",
      "'RandomForestRegressor' -> avg R2 'incompetent' = -0.02\n",
      "'RandomForestRegressor' -> avg RMSE 'ugly' = 8.50\n",
      "'RandomForestRegressor' -> avg R2 'ugly' = 0.19\n",
      "'RandomForestRegressor' -> avg RMSE 'friendly' = 8.83\n",
      "'RandomForestRegressor' -> avg R2 'friendly' = 0.17\n",
      "'RandomForestRegressor' -> avg RMSE 'masculine' = 6.47\n",
      "'RandomForestRegressor' -> avg R2 'masculine' = 0.96\n",
      "'RandomForestRegressor' -> avg RMSE 'submissive' = 8.21\n",
      "'RandomForestRegressor' -> avg R2 'submissive' = -0.01\n",
      "'RandomForestRegressor' -> avg RMSE 'indifferent' = 7.92\n",
      "'RandomForestRegressor' -> avg R2 'indifferent' = 0.40\n",
      "'RandomForestRegressor' -> avg RMSE 'interesting' = 8.45\n",
      "'RandomForestRegressor' -> avg R2 'interesting' = 0.12\n",
      "'RandomForestRegressor' -> avg RMSE 'cynical' = 7.68\n",
      "'RandomForestRegressor' -> avg R2 'cynical' = 0.01\n",
      "'RandomForestRegressor' -> avg RMSE 'artificial' = 7.66\n",
      "'RandomForestRegressor' -> avg R2 'artificial' = 0.01\n",
      "'RandomForestRegressor' -> avg RMSE 'intelligent' = 8.22\n",
      "'RandomForestRegressor' -> avg R2 'intelligent' = -0.02\n",
      "'RandomForestRegressor' -> avg RMSE 'childish' = 10.54\n",
      "'RandomForestRegressor' -> avg R2 'childish' = 0.14\n",
      "'RandomForestRegressor' -> avg RMSE 'modest' = 8.50\n",
      "'RandomForestRegressor' -> avg R2 'modest' = -0.05\n",
      "'RandomForestRegressor' -> avg R2 overall: 0.15\n",
      "\n",
      "'DecisionTreeRegressor' -> avg RMSE 'non_likable' = 10.24\n",
      "'DecisionTreeRegressor' -> avg R2 'non_likable' = 0.08\n",
      "'DecisionTreeRegressor' -> avg RMSE 'secure' = 11.03\n",
      "'DecisionTreeRegressor' -> avg R2 'secure' = 0.04\n",
      "'DecisionTreeRegressor' -> avg RMSE 'attractive' = 10.17\n",
      "'DecisionTreeRegressor' -> avg R2 'attractive' = -0.03\n",
      "'DecisionTreeRegressor' -> avg RMSE 'unsympathetic' = 7.26\n",
      "'DecisionTreeRegressor' -> avg R2 'unsympathetic' = 0.12\n",
      "'DecisionTreeRegressor' -> avg RMSE 'indecisive' = 9.78\n",
      "'DecisionTreeRegressor' -> avg R2 'indecisive' = -0.00\n",
      "'DecisionTreeRegressor' -> avg RMSE 'unobtrusive' = 9.03\n",
      "'DecisionTreeRegressor' -> avg R2 'unobtrusive' = -0.17\n",
      "'DecisionTreeRegressor' -> avg RMSE 'distant' = 9.47\n",
      "'DecisionTreeRegressor' -> avg R2 'distant' = 0.11\n",
      "'DecisionTreeRegressor' -> avg RMSE 'bored' = 9.47\n",
      "'DecisionTreeRegressor' -> avg R2 'bored' = 0.28\n",
      "'DecisionTreeRegressor' -> avg RMSE 'emotional' = 8.10\n",
      "'DecisionTreeRegressor' -> avg R2 'emotional' = 0.35\n",
      "'DecisionTreeRegressor' -> avg RMSE 'not_irritated' = 10.20\n",
      "'DecisionTreeRegressor' -> avg R2 'not_irritated' = -0.07\n",
      "'DecisionTreeRegressor' -> avg RMSE 'active' = 9.88\n",
      "'DecisionTreeRegressor' -> avg R2 'active' = 0.13\n",
      "'DecisionTreeRegressor' -> avg RMSE 'pleasant' = 9.79\n",
      "'DecisionTreeRegressor' -> avg R2 'pleasant' = 0.07\n",
      "'DecisionTreeRegressor' -> avg RMSE 'characterless' = 7.88\n",
      "'DecisionTreeRegressor' -> avg R2 'characterless' = 0.14\n",
      "'DecisionTreeRegressor' -> avg RMSE 'sociable' = 9.09\n",
      "'DecisionTreeRegressor' -> avg R2 'sociable' = 0.19\n",
      "'DecisionTreeRegressor' -> avg RMSE 'relaxed' = 9.42\n",
      "'DecisionTreeRegressor' -> avg R2 'relaxed' = 0.02\n",
      "'DecisionTreeRegressor' -> avg RMSE 'affectionate' = 9.36\n",
      "'DecisionTreeRegressor' -> avg R2 'affectionate' = 0.21\n",
      "'DecisionTreeRegressor' -> avg RMSE 'dominant' = 9.70\n",
      "'DecisionTreeRegressor' -> avg R2 'dominant' = -0.06\n",
      "'DecisionTreeRegressor' -> avg RMSE 'unaffected' = 8.07\n",
      "'DecisionTreeRegressor' -> avg R2 'unaffected' = 0.00\n",
      "'DecisionTreeRegressor' -> avg RMSE 'hearty' = 9.72\n",
      "'DecisionTreeRegressor' -> avg R2 'hearty' = 0.19\n",
      "'DecisionTreeRegressor' -> avg RMSE 'old' = 12.30\n",
      "'DecisionTreeRegressor' -> avg R2 'old' = 0.07\n",
      "'DecisionTreeRegressor' -> avg RMSE 'personal' = 7.16\n",
      "'DecisionTreeRegressor' -> avg R2 'personal' = 0.04\n",
      "'DecisionTreeRegressor' -> avg RMSE 'calm' = 10.23\n",
      "'DecisionTreeRegressor' -> avg R2 'calm' = 0.08\n",
      "'DecisionTreeRegressor' -> avg RMSE 'incompetent' = 7.99\n",
      "'DecisionTreeRegressor' -> avg R2 'incompetent' = 0.02\n",
      "'DecisionTreeRegressor' -> avg RMSE 'ugly' = 8.93\n",
      "'DecisionTreeRegressor' -> avg R2 'ugly' = 0.11\n",
      "'DecisionTreeRegressor' -> avg RMSE 'friendly' = 9.03\n",
      "'DecisionTreeRegressor' -> avg R2 'friendly' = 0.13\n",
      "'DecisionTreeRegressor' -> avg RMSE 'masculine' = 6.10\n",
      "'DecisionTreeRegressor' -> avg R2 'masculine' = 0.96\n",
      "'DecisionTreeRegressor' -> avg RMSE 'submissive' = 8.19\n",
      "'DecisionTreeRegressor' -> avg R2 'submissive' = -0.00\n",
      "'DecisionTreeRegressor' -> avg RMSE 'indifferent' = 8.26\n",
      "'DecisionTreeRegressor' -> avg R2 'indifferent' = 0.34\n",
      "'DecisionTreeRegressor' -> avg RMSE 'interesting' = 8.48\n",
      "'DecisionTreeRegressor' -> avg R2 'interesting' = 0.11\n",
      "'DecisionTreeRegressor' -> avg RMSE 'cynical' = 8.21\n",
      "'DecisionTreeRegressor' -> avg R2 'cynical' = -0.13\n",
      "'DecisionTreeRegressor' -> avg RMSE 'artificial' = 7.64\n",
      "'DecisionTreeRegressor' -> avg R2 'artificial' = 0.01\n",
      "'DecisionTreeRegressor' -> avg RMSE 'intelligent' = 8.23\n",
      "'DecisionTreeRegressor' -> avg R2 'intelligent' = -0.03\n",
      "'DecisionTreeRegressor' -> avg RMSE 'childish' = 10.38\n",
      "'DecisionTreeRegressor' -> avg R2 'childish' = 0.17\n",
      "'DecisionTreeRegressor' -> avg RMSE 'modest' = 8.10\n",
      "'DecisionTreeRegressor' -> avg R2 'modest' = 0.05\n",
      "'DecisionTreeRegressor' -> avg R2 overall: 0.10\n",
      "\n",
      "'DummyRegressor' -> avg RMSE 'non_likable' = 10.68\n",
      "'DummyRegressor' -> avg R2 'non_likable' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'secure' = 11.38\n",
      "'DummyRegressor' -> avg R2 'secure' = -0.02\n",
      "'DummyRegressor' -> avg RMSE 'attractive' = 10.07\n",
      "'DummyRegressor' -> avg R2 'attractive' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'unsympathetic' = 7.79\n",
      "'DummyRegressor' -> avg R2 'unsympathetic' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'indecisive' = 9.83\n",
      "'DummyRegressor' -> avg R2 'indecisive' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'unobtrusive' = 8.58\n",
      "'DummyRegressor' -> avg R2 'unobtrusive' = -0.05\n",
      "'DummyRegressor' -> avg RMSE 'distant' = 10.10\n",
      "'DummyRegressor' -> avg R2 'distant' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'bored' = 11.12\n",
      "'DummyRegressor' -> avg R2 'bored' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'emotional' = 10.09\n",
      "'DummyRegressor' -> avg R2 'emotional' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'not_irritated' = 9.88\n",
      "'DummyRegressor' -> avg R2 'not_irritated' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'active' = 10.69\n",
      "'DummyRegressor' -> avg R2 'active' = -0.02\n",
      "'DummyRegressor' -> avg RMSE 'pleasant' = 10.19\n",
      "'DummyRegressor' -> avg R2 'pleasant' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'characterless' = 8.58\n",
      "'DummyRegressor' -> avg R2 'characterless' = -0.02\n",
      "'DummyRegressor' -> avg RMSE 'sociable' = 10.23\n",
      "'DummyRegressor' -> avg R2 'sociable' = -0.02\n",
      "'DummyRegressor' -> avg RMSE 'relaxed' = 9.54\n",
      "'DummyRegressor' -> avg R2 'relaxed' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'affectionate' = 10.50\n",
      "'DummyRegressor' -> avg R2 'affectionate' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'dominant' = 9.74\n",
      "'DummyRegressor' -> avg R2 'dominant' = -0.07\n",
      "'DummyRegressor' -> avg RMSE 'unaffected' = 8.09\n",
      "'DummyRegressor' -> avg R2 'unaffected' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'hearty' = 10.81\n",
      "'DummyRegressor' -> avg R2 'hearty' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'old' = 12.75\n",
      "'DummyRegressor' -> avg R2 'old' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'personal' = 7.34\n",
      "'DummyRegressor' -> avg R2 'personal' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'calm' = 10.68\n",
      "'DummyRegressor' -> avg R2 'calm' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'incompetent' = 8.17\n",
      "'DummyRegressor' -> avg R2 'incompetent' = -0.03\n",
      "'DummyRegressor' -> avg RMSE 'ugly' = 9.47\n",
      "'DummyRegressor' -> avg R2 'ugly' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'friendly' = 9.69\n",
      "'DummyRegressor' -> avg R2 'friendly' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'masculine' = 31.29\n",
      "'DummyRegressor' -> avg R2 'masculine' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'submissive' = 8.32\n",
      "'DummyRegressor' -> avg R2 'submissive' = -0.03\n",
      "'DummyRegressor' -> avg RMSE 'indifferent' = 10.22\n",
      "'DummyRegressor' -> avg R2 'indifferent' = -0.01\n",
      "'DummyRegressor' -> avg RMSE 'interesting' = 9.15\n",
      "'DummyRegressor' -> avg R2 'interesting' = -0.03\n",
      "'DummyRegressor' -> avg RMSE 'cynical' = 7.95\n",
      "'DummyRegressor' -> avg R2 'cynical' = -0.06\n",
      "'DummyRegressor' -> avg RMSE 'artificial' = 7.68\n",
      "'DummyRegressor' -> avg R2 'artificial' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'intelligent' = 8.23\n",
      "'DummyRegressor' -> avg R2 'intelligent' = -0.03\n",
      "'DummyRegressor' -> avg RMSE 'childish' = 11.40\n",
      "'DummyRegressor' -> avg R2 'childish' = -0.00\n",
      "'DummyRegressor' -> avg RMSE 'modest' = 8.55\n",
      "'DummyRegressor' -> avg R2 'modest' = -0.06\n",
      "'DummyRegressor' -> avg R2 overall: -0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# go through performace for all regressors\n",
    "\n",
    "# removing duplicates from tuning_all (same classifier tuned twice with different searchers)\n",
    "indexes = tuning_all['regressors_names'].drop_duplicates(keep='last').index.values\n",
    "\n",
    "# dataframe for summary of performances\n",
    "# performances = pd.DataFrame(tuning_all.loc[indexes,['regressors_names','best_accs']])\n",
    "\n",
    "for i in indexes:\n",
    "\n",
    "    # compute predictions with the best tuned regressor\n",
    "\n",
    "    yt_pred = trained_all[i][0].predict(Xt)\n",
    "\n",
    "    # average of outputs that belong to the same speaker\n",
    "\n",
    "    true_scores = pd.DataFrame(data = feats_ratings_scores_test[items_names+['spkID']])\n",
    "    true_scores['type']='true'\n",
    "\n",
    "    pred_scores=pd.DataFrame()\n",
    "    for t in np.arange(0,len(items_names)):\n",
    "        pred_scores[items_names[t]] = yt_pred[:, t] \n",
    "    pred_scores['spkID'] = feats_ratings_scores_test['spkID']\n",
    "\n",
    "\n",
    "    # group by speakers and average\n",
    "    true_scores_avg = true_scores.groupby('spkID').mean()\n",
    "\n",
    "    pred_scores_avg = pred_scores.groupby('spkID').mean()\n",
    "\n",
    "\n",
    "    # RMSE and R2 for each trait separately\n",
    "    for t in np.arange(0,len(items_names)):\n",
    "        print('%r -> avg RMSE %r = %.2f' % (tuning_all.loc[i,'regressors_names'],\n",
    "                                      items_names[t], \n",
    "                                      np.sqrt(mean_squared_error(true_scores_avg[items_names[t]].as_matrix(), \n",
    "                                                                                  pred_scores_avg[items_names[t]].as_matrix()))\n",
    "                                     )\n",
    "             )\n",
    "        \n",
    "        print('%r -> avg R2 %r = %.2f' % (tuning_all.loc[i,'regressors_names'],\n",
    "                                      items_names[t], \n",
    "                                      r2_score(true_scores_avg[items_names[t]].as_matrix(), \n",
    "                                                                                  pred_scores_avg[items_names[t]].as_matrix())\n",
    "                                     )\n",
    "             )\n",
    "\n",
    "    # overall RMSE and R2\n",
    "    myrmse_avg = np.sqrt(mean_squared_error(true_scores_avg[items_names].as_matrix(), \n",
    "                                            pred_scores_avg[items_names].as_matrix())\n",
    "                        )\n",
    "    myr2_avg = r2_score(true_scores_avg[items_names].as_matrix(), \n",
    "                                            pred_scores_avg[items_names].as_matrix()\n",
    "                        )\n",
    "    print('%r -> avg R2 overall: %0.2f' % (tuning_all.loc[i,'regressors_names'], myr2_avg))\n",
    "    print('')\n",
    "    \n",
    "        \n",
    "    # append true and predicted scores\n",
    "\n",
    "    true_scores_avg.reset_index(inplace=True)\n",
    "    pred_scores_avg.reset_index(inplace=True) \n",
    "\n",
    "    true_scores_avg['type']='true'\n",
    "    pred_scores_avg['type']='pred'\n",
    "\n",
    "    test_scores_avg=true_scores_avg.append(pred_scores_avg)\n",
    "\n",
    "#     # pairplot color-coded by true/predicted test data\n",
    "#     myfig = sns.pairplot(test_scores_avg.drop('spkID', axis=1), hue='type')\n",
    "\n",
    "#     # save figure\n",
    "#     filename = label + '_test_'+tuning_all.loc[i,'regressors_names']+'.png'\n",
    "#     myfig.savefig('.\\\\figures\\\\' + filename, bbox_inches = 'tight')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x1ef7fce3128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAFgCAYAAACYM1+SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef7eca0978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example predictions made by RandomForestRegressor\n",
    "\n",
    "sns.pairplot(test_scores_avg[['type','old','calm']], hue='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation (.html) of pairs of true-predicted WAAT scores. For each frame i, 2 points are plotted in 2D: one corresponding to the true WAAT of speaker i and the other one corresponding to the averaged predicted WAAT of the same speaker.\n",
    "\n",
    "TODO: Select test_scores_avg corresponding to the regressor to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# unique_speakers = test_scores_avg['spkID'].unique()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot a scatter that persists(isn't redrawn) \n",
    "# ax.set_xlabel('warmth')\n",
    "# ax.set_ylabel('attractiveness')\n",
    "# ax.set_xlim(-6, 4)  \n",
    "# ax.set_ylim(-3, 2)        \n",
    "\n",
    "# def update(i):\n",
    "#     spk=unique_speakers[i]\n",
    "#     coor_x = test_scores_avg.loc[test_scores_avg['spkID']==spk, traits_names[0]]\n",
    "#     coor_y = test_scores_avg.loc[test_scores_avg['spkID']==spk, traits_names[1]]\n",
    "#     ax.scatter(coor_x, coor_y)\n",
    "#     ax.plot(coor_x, coor_y) \n",
    "#     return ax\n",
    "\n",
    "# # create animation\n",
    "# anim = FuncAnimation(fig, update, frames=np.arange(0, len(unique_speakers)), interval=200, save_count=200)\n",
    "\n",
    "# # save animation\n",
    "# filename = r'\\multioutput_test_'+tuning_all.loc[i,'regressors_names']+'.html'\n",
    "# anim.save(r'.\\figures' + filename, dpi=80, writer='imagemagick')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
