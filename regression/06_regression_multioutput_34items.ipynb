{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating regression techniques for speaker characterization\n",
    "### Laura Fern√°ndez Gallardo\n",
    "\n",
    "Multioutput regression.\n",
    "\n",
    "Motifications with respect to regression with 1-dimensional output:\n",
    "\n",
    "Targets: 34-dimensional ratings of [speaker characteristics](https://github.com/laufergall/Subjective_Speaker_Characteristics) given to the 300 speakers of the [NSC](http://www.qu.tu-berlin.de/?id=nsc-corpus) corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from reg_tuning import * # my helper functions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 2302\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/laufergall/ML_Speaker_Characteristics/master/data/generated_data/'\n",
    "\n",
    "url = path + \"feats_ratings_scores_train.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_train = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "url = path + \"feats_ratings_scores_test.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_test = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "with open(r'..\\data\\generated_data\\feats_names.txt') as f:\n",
    "    feats_names = f.readlines()\n",
    "feats_names = [x.strip().strip('\\'') for x in feats_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\items_names.txt') as f:\n",
    "    items_names = f.readlines()\n",
    "items_names = [x.strip().strip('\\'') for x in items_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\traits_names.txt') as f:\n",
    "    traits_names = f.readlines()\n",
    "traits_names = [x.strip().strip('\\'') for x in traits_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34items: Model tuning\n",
    "\n",
    "Use the train data to find the regressor and its hyperparameters leading to the best performance. \n",
    "\n",
    "Not performing feature selection with \"SelectKBest\": Univariate feature selection does not support multilabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize speech features  \n",
    "\n",
    "dropcolumns = ['name','spkID','speaker_gender'] + items_names + traits_names\n",
    "\n",
    "# learn transformation on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats_ratings_scores_train.drop(dropcolumns, axis=1))\n",
    "\n",
    "# numpy n_instances x n_feats\n",
    "feats_s_train = scaler.transform(feats_ratings_scores_train.drop(dropcolumns, axis=1))\n",
    "feats_s_test = scaler.transform(feats_ratings_scores_test.drop(dropcolumns, axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data. Features and labels\n",
    "X = feats_s_train # (2700, 88)\n",
    "y = feats_ratings_scores_train[items_names].as_matrix() # (2700, 34)\n",
    "\n",
    "# test data. Features and labels\n",
    "Xt = feats_s_test # (891, 88)\n",
    "yt = feats_ratings_scores_test[items_names].as_matrix() # (891, 34)\n",
    "\n",
    "# split train data into 80% and 20% subsets - with balance in trait and gender\n",
    "# give subset A to the inner hyperparameter tuner\n",
    "# and hold out subset B for meta-evaluation\n",
    "AX, BX, Ay, By = train_test_split(X, y, \n",
    "                                  test_size=0.20, \n",
    "                                  stratify = feats_ratings_scores_train['speaker_gender'], \n",
    "                                  random_state=2302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe with results from hp tuner to be appended\n",
    "tuning_all = pd.DataFrame()\n",
    "\n",
    "# list with tuned models trained on training data, to be appended\n",
    "trained_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save splits\n",
    "\n",
    "label = 'multioutput_34items'\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "np.save(r'.\\data_while_tuning\\X_' + label + '.npy', X)\n",
    "np.save(r'.\\data_while_tuning\\y_' + label + '.npy', y)\n",
    "np.save(r'.\\data_while_tuning\\Xt_' + label + '.npy', Xt)\n",
    "np.save(r'.\\data_while_tuning\\yt_' + label + '.npy', yt)\n",
    "\n",
    "# # A/B splits, features and labels\n",
    "np.save(r'.\\data_while_tuning\\AX_' + label + '.npy', AX)\n",
    "np.save(r'.\\data_while_tuning\\BX_' + label + '.npy', BX)\n",
    "np.save(r'.\\data_while_tuning\\Ay_' + label + '.npy', Ay)\n",
    "np.save(r'.\\data_while_tuning\\By_' + label + '.npy', By)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling hp_tuner() for each regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Recover ** when new ipynb session started.\n",
    "\n",
    "(Workaround for working with hyperparameter tuning during several days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label = 'multioutput_34items'\n",
    "\n",
    "# # train/test partitions, features and labels\n",
    "# X = np.load(r'.\\data_while_tuning\\X_' + label + '.npy')\n",
    "# y = np.load(r'.\\data_while_tuning\\y_' + label + '.npy')\n",
    "# Xt = np.load(r'.\\data_while_tuning\\Xt_' + label + '.npy')\n",
    "# yt = np.load(r'.\\data_while_tuning\\yt_' + label + '.npy')\n",
    "\n",
    "# # A/B splits, features and labels\n",
    "# AX = np.load(r'.\\data_while_tuning\\AX_' + label + '.npy')\n",
    "# BX = np.load(r'.\\data_while_tuning\\BX_' + label + '.npy')\n",
    "# Ay = np.load(r'.\\data_while_tuning\\Ay_' + label + '.npy')\n",
    "# By = np.load(r'.\\data_while_tuning\\By_' + label + '.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Loading outpus of hp tuning from disk\n",
    "# tuning_all, trained_all = load_tuning(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this after each experiment **to recover later**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # save tuning_all (.csv) and trained_all (namemodel.sav)\n",
    "# save_tuning(tuning_all, trained_all, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multilayer Perceptron for regression\n",
    "\n",
    "Defining model architecture: Multilayer Perceptron (MLP) using keras with TensorFlow backend.\n",
    "\n",
    "* input the (standardized) speech features (input_dim = 88)\n",
    "* one fully-connected hidden layer with the same number of neurons as input speech features\n",
    "* output with linear activation function (for regession) with the same number of neurons as output speaker characteristics\n",
    "\n",
    "Tuning hyperparameters using StratifiedKFold cross-validation with set A:\n",
    "\n",
    "* number of epochs\n",
    "* batch size\n",
    "* number of hidden units\n",
    "* optimizer\n",
    "* learning rate\n",
    "* activation function hidden layer\n",
    "* dropout regularization\n",
    "\n",
    "Evaluating performance on the hold-out set B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\"\"\"\n",
    "MLP with KerasRegressor\n",
    "\"\"\"\n",
    "\n",
    "def create_model(optimizer = 'Adam', learn_rate=0.2, neurons=1, activation='relu', dropout_rate=0.0):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons,\n",
    "                    activation=activation, \n",
    "                    input_dim=88))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(34))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laura\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:115: DeprecationWarning: Estimator KerasRegressor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (34,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-cff6bc300c6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                            \u001b[1;33m[\u001b[0m\u001b[1;36m88\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# no feature selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                            \u001b[1;34m'random'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                            \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                           )\n",
      "\u001b[1;32mD:\\Work\\git_ML\\ML_Speaker_Characteristics\\regression\\reg_tuning.py\u001b[0m in \u001b[0;36mhp_tuner\u001b[1;34m(AX, BX, Ay, By, get_reg_functions, label, feats_names, k_array, mode, n_iter)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m# This might take a while:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0msearch_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m# summary of hp tuning on set A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    168\u001b[0m             delayed(_fit_estimator)(\n\u001b[0;32m    169\u001b[0m                 self.estimator, X, y[:, i], sample_weight)\n\u001b[1;32m--> 170\u001b[1;33m             for i in range(y.shape[1]))\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1591\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1593\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1594\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1431\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1432\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    118\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (34,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# 1st round (tuning epochs, batch_size, neurons, learn rate):\n",
    "    \n",
    "def get_KerasRegressor2tune():\n",
    "    \n",
    "    model = KerasRegressor(build_fn = create_model, verbose=0)\n",
    "                        \n",
    "    hp = dict(\n",
    "        regressor__estimator__epochs = [25,50,75,100],\n",
    "        regressor__estimator__batch_size = [5,10], \n",
    "        regressor__estimator__neurons = [40, 60, 80, 160],\n",
    "        regressor__estimator__learn_rate = np.arange(start=0.2, stop=1.0, step=0.05) \n",
    "        #regressor__estimator__activation = ['relu'], # ['softmax', 'softplus', 'sofsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "        #regressor__estimator__dropout_rate = [0.5], # np.arange(start=0, stop=1, step=0.1)\n",
    "        #regressor__estimator__optimizer = ['Adam'] #['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    )\n",
    "    return 'KerasRegressor', model, hp\n",
    "\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_KerasRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # no feature selection\n",
    "                           'random',\n",
    "                           n_iter=20\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 2nd round (tuning activation and dropout_rate):\n",
    "    \n",
    "# def get_KerasRegressor2tune():\n",
    "    \n",
    "#     model = KerasRegressor(build_fn = create_model, verbose=0)\n",
    "                        \n",
    "#     hp = dict(\n",
    "#         regressor__estimator__epochs = [75], #[25,50,75,100],\n",
    "#         regressor__estimator__batch_size = [5], # [5,10], \n",
    "#         regressor__estimator__neurons = [160], #[40, 80, 160],\n",
    "#         regressor__estimator__learn_rate = [0.8], # np.arange(start=0.2, stop=1.0, step=0.05) \n",
    "#         regressor__estimator__activation = ['relu','tanh','sigmoid','linear'], # ['softmax', 'softplus', 'sofsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "#         regressor__estimator__dropout_rate = np.arange(start=0, stop=1, step=0.2)\n",
    "#         #regressor__estimator__optimizer = ['Adam'] #['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#     )\n",
    "#     return 'KerasRegressor', model, hp\n",
    "\n",
    "\n",
    "# tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "#                            [get_KerasRegressor2tune], \n",
    "#                            label,\n",
    "#                            feats_names,\n",
    "#                            [88], # no feature selection\n",
    "#                            'grid'\n",
    "#                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree\n",
    "\n",
    "*class sklearn.tree.DecisionTreeRegressor(criterion=‚Äômse‚Äô, splitter=‚Äôbest‚Äô, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False)*\n",
    "\n",
    "Tune: max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\"\"\"\n",
    "Decision Trees\n",
    "\"\"\"\n",
    "def get_DecisionTreeRegressor2tune():\n",
    "    \n",
    "    model = DecisionTreeRegressor()\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_depth = np.arange(2,20) \n",
    "    )\n",
    "    return 'DecisionTreeRegressor', model, hp\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_DecisionTreeRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='grid'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "*class sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=‚Äômse‚Äô, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=‚Äôauto‚Äô, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)*\n",
    "\n",
    "Tune: max_features, max_depth, min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "def get_RandomForestRegressor2tune():\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=2302, max_features = None)\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_features = np.arange(2,50),\n",
    "        regressor__estimator__max_depth = np.arange(2,50), \n",
    "        regressor__estimator__min_samples_leaf = np.arange(2,50) \n",
    "    )\n",
    "    return 'RandomForestRegressor', model, hp\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_RandomForestRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='random',\n",
    "                           n_iter=50\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "def get_RandomForestRegressor2tune():\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=2302, max_features = None)\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_features = np.arange(25,45),\n",
    "        regressor__estimator__max_depth = np.arange(25,45), \n",
    "        regressor__estimator__min_samples_leaf = np.arange(4,7) \n",
    "    )\n",
    "    return 'RandomForestRegressor', model, hp\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_RandomForestRegressor2tune], \n",
    "                           label,\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='random',\n",
    "                           n_iter=50\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update lists of tuning info and trained regressors\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)\n",
    "\n",
    "# save tuning_all (.csv) and trained_all (nameregressor.sav)\n",
    "save_tuning(tuning_all, trained_all, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "model = DummyRegressor(strategy='mean')\n",
    "model.fit(AX, Ay)\n",
    "By_pred = model.predict(BX)\n",
    "score_on_B = np.sqrt(mean_squared_error(By, By_pred))\n",
    "d = {\n",
    "    'regressors_names': ['DummyRegressor'],\n",
    "    'best_accs': score_on_B,\n",
    "    'best_hps': '',\n",
    "    'sel_feats': '',\n",
    "    'sel_feats_i': ''\n",
    "    }\n",
    "\n",
    "tuning = pd.DataFrame(data = d)\n",
    "trained = model.fit(X, y)\n",
    "\n",
    "# update lists of tuning info and trained regressors\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append([trained])\n",
    "\n",
    "# save tuning_all (.csv) and trained_all (nameregressor.sav)\n",
    "save_tuning(tuning_all, trained_all, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### RMSE\n",
    "Testing the best model found after hyperparameter tuning.\n",
    "\n",
    "Performance metric: RMSE.\n",
    "    \n",
    "The predictions correspond to the scores of each test segment (3 parts x 4 dialogs) spoken by the same test speaker. I perform the average of the predicted scores that correspond to the same speaker - to be compared to the true scores.\n",
    "\n",
    "Visualization: pairplot of scores of true and predicted test data, grouped-averaged by speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data if new ipynb session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/laufergall/ML_Speaker_Characteristics/master/data/generated_data/'\n",
    "\n",
    "url = path + \"feats_ratings_scores_train.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_train = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "url = path + \"feats_ratings_scores_test.csv\"\n",
    "s = requests.get(url).content\n",
    "feats_ratings_scores_test = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "with open(r'..\\data\\generated_data\\feats_names.txt') as f:\n",
    "    feats_names = f.readlines()\n",
    "feats_names = [x.strip().strip('\\'') for x in feats_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\items_names.txt') as f:\n",
    "    items_names = f.readlines()\n",
    "items_names = [x.strip().strip('\\'') for x in items_names] \n",
    "\n",
    "with open(r'..\\data\\generated_data\\traits_names.txt') as f:\n",
    "    traits_names = f.readlines()\n",
    "traits_names = [x.strip().strip('\\'') for x in traits_names] \n",
    "\n",
    "label = 'multioutput_5traits'\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "X = np.load(r'.\\data_while_tuning\\X_' + label + '.npy')\n",
    "y = np.load(r'.\\data_while_tuning\\y_' + label + '.npy')\n",
    "Xt = np.load(r'.\\data_while_tuning\\Xt_' + label + '.npy')\n",
    "yt = np.load(r'.\\data_while_tuning\\yt_' + label + '.npy')\n",
    "\n",
    "# Loading outpus of hp tuning from disk\n",
    "tuning_all, trained_all = load_tuning(label)\n",
    "tuning_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the classifier that gave the maximum acc on B set\n",
    "best_accs = tuning_all['best_accs']\n",
    "i_best = best_accs.idxmin()\n",
    "\n",
    "print('Selected classifier based on the best performance on B: %r (perf. on B = %0.2f)' % (tuning_all.loc[i_best,'regressors_names'], round(best_accs[i_best],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# go through performace for all regressors\n",
    "\n",
    "# removing duplicates from tuning_all (same classifier tuned twice with different searchers)\n",
    "indexes = tuning_all['regressors_names'].drop_duplicates(keep='last').index.values\n",
    "\n",
    "# dataframe for summary of performances\n",
    "# performances = pd.DataFrame(tuning_all.loc[indexes,['regressors_names','best_accs']])\n",
    "\n",
    "for i in indexes:\n",
    "\n",
    "    # compute predictions with the best tuned regressor\n",
    "\n",
    "    yt_pred = trained_all[i][0].predict(Xt)\n",
    "\n",
    "    # average of outputs that belong to the same speaker\n",
    "\n",
    "    true_scores = pd.DataFrame(data = feats_ratings_scores_test[traits_names+['spkID']])\n",
    "    true_scores['type']='true'\n",
    "\n",
    "    pred_scores=pd.DataFrame()\n",
    "    for t in np.arange(0,len(traits_names)):\n",
    "        pred_scores[traits_names[t]] = yt_pred[:, t] \n",
    "    pred_scores['spkID'] = feats_ratings_scores_test['spkID']\n",
    "\n",
    "\n",
    "    # group by speakers and average\n",
    "    true_scores_avg = true_scores.groupby('spkID').mean()\n",
    "\n",
    "    pred_scores_avg = pred_scores.groupby('spkID').mean()\n",
    "\n",
    "\n",
    "    # RMSE for each trait separately\n",
    "    for t in np.arange(0,len(traits_names)):\n",
    "        print('%r -> avg RMSE %r = %.2f' % (tuning_all.loc[i,'regressors_names'],\n",
    "                                      traits_names[t], \n",
    "                                      np.sqrt(mean_squared_error(true_scores_avg[traits_names[t]].as_matrix(), \n",
    "                                                                                  pred_scores_avg[traits_names[t]].as_matrix()))\n",
    "                                     )\n",
    "             )\n",
    "\n",
    "    # overall RMSE\n",
    "    myrmse_avg = np.sqrt(mean_squared_error(true_scores_avg[traits_names].as_matrix(), \n",
    "                                            pred_scores_avg[traits_names].as_matrix())\n",
    "                        )\n",
    "\n",
    "    print('%r -> avg RMSE overall: %0.2f' % (tuning_all.loc[i,'regressors_names'], myrmse_avg))\n",
    "    print('')\n",
    "    \n",
    "    # append true and predicted scores\n",
    "\n",
    "    true_scores_avg.reset_index(inplace=True)\n",
    "    pred_scores_avg.reset_index(inplace=True) \n",
    "\n",
    "    true_scores_avg['type']='true'\n",
    "    pred_scores_avg['type']='pred'\n",
    "\n",
    "    test_scores_avg=true_scores_avg.append(pred_scores_avg)\n",
    "\n",
    "    # pairplot color-coded by true/predicted test data\n",
    "    myfig = sns.pairplot(test_scores_avg.drop('spkID', axis=1), hue='type')\n",
    "\n",
    "    # save figure\n",
    "    filename = r'\\multioutput_test_'+tuning_all.loc[i,'regressors_names']+'.png'\n",
    "    myfig.savefig(r'.\\figures' + filename, bbox_inches = 'tight')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation (.html) of pairs of true-predicted WAAT scores. For each frame i, 2 points are plotted in 2D: one corresponding to the true WAAT of speaker i and the other one corresponding to the averaged predicted WAAT of the same speaker.\n",
    "\n",
    "TODO: Select test_scores_avg corresponding to the regressor to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# unique_speakers = test_scores_avg['spkID'].unique()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot a scatter that persists(isn't redrawn) \n",
    "# ax.set_xlabel('warmth')\n",
    "# ax.set_ylabel('attractiveness')\n",
    "# ax.set_xlim(-6, 4)  \n",
    "# ax.set_ylim(-3, 2)        \n",
    "\n",
    "# def update(i):\n",
    "#     spk=unique_speakers[i]\n",
    "#     coor_x = test_scores_avg.loc[test_scores_avg['spkID']==spk, traits_names[0]]\n",
    "#     coor_y = test_scores_avg.loc[test_scores_avg['spkID']==spk, traits_names[1]]\n",
    "#     ax.scatter(coor_x, coor_y)\n",
    "#     ax.plot(coor_x, coor_y) \n",
    "#     return ax\n",
    "\n",
    "# # create animation\n",
    "# anim = FuncAnimation(fig, update, frames=np.arange(0, len(unique_speakers)), interval=200, save_count=200)\n",
    "\n",
    "# # save animation\n",
    "# filename = r'\\multioutput_test_'+tuning_all.loc[i,'regressors_names']+'.html'\n",
    "# anim.save(r'.\\figures' + filename, dpi=80, writer='imagemagick')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
