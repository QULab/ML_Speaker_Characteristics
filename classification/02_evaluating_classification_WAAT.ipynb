{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classification techniques for speaker characterization\n",
    "### Laura FernÃ¡ndez Gallardo\n",
    "\n",
    "In this notebook, I will evaluate the performance of different classification techniques for characterizing users' warmth-attractiveness (WAAT). \n",
    "\n",
    "* Thesholding continuous scores generated from [factor analysis](https://github.com/laufergall/Subjective_Speaker_Characteristics/tree/master/speaker_characteristics/factor_analysis) based on percentiles to define 3 classes (\"high\", \"mid\", and \"low\") with approximately the same number of samples.\n",
    "* Only the \"high\" and \"low\" classes are of interest -> I address **binary classification**.\n",
    "* As evaluation metric, I will consider the average per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import time # for timestamps\n",
    "import pickle # save models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 2302\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speakers' WAAT\n",
    "\n",
    "WAAT (warmth-attractiveness) can be seen as the first two dimensions of the perceived speaker characteristics. Scaled (with mean = 0 and std = 1) scores of speakers on these dimensions were already extracted in the [subjective analysis](https://github.com/laufergall/Subjective_Speaker_Characteristics), for males and for females separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load WAAT scores (averaged across listeners)\n",
    "\n",
    "path = \"https://raw.githubusercontent.com/laufergall/Subjective_Speaker_Characteristics/master/data/generated_data/\"\n",
    "\n",
    "url = path + \"factorscores_malespk.csv\"\n",
    "s = requests.get(url).content\n",
    "scores_m =pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "url = path + \"factorscores_femalespk.csv\"\n",
    "s = requests.get(url).content\n",
    "scores_f =pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "# rename dimensions\n",
    "scores_m.columns = ['sample_heard', 'warmth', 'attractiveness', 'confidence', 'compliance', 'maturity']\n",
    "scores_f.columns = ['sample_heard', 'warmth', 'attractiveness', 'compliance', 'confidence', 'maturity']\n",
    "\n",
    "# join male and feame scores\n",
    "scores = scores_m.append(scores_f)\n",
    "scores['gender'] = scores['sample_heard'].str.slice(0,1)\n",
    "scores['spkID'] = scores['sample_heard'].str.slice(1,4).astype('int')\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "\n",
    "sns.lmplot('warmth', 'attractiveness', data = scores, hue=\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram, kernel density estimation\n",
    "sns.jointplot('warmth', 'attractiveness', data = scores, kind=\"kde\").set_axis_labels(\"warmth\", \"attractiveness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 3 clusters of speakers based on the WAAT distribution.\n",
    "Each cluster with approx. the same number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying k-means\n",
    "\n",
    "n_clusters=3\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=2302).fit(scores[['warmth','attractiveness']])\n",
    "\n",
    "scores['class'] = pd.Categorical(kmeans.labels_).rename_categories(['low','high','mid'])\n",
    "\n",
    "sns.lmplot('warmth', 'attractiveness', data = scores, hue=\"class\",fit_reg=False)\n",
    " \n",
    "print(scores['class'].value_counts())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select trait for binary classification and perform data partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing** speakers in the mid class to address binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove speakers in the mid class\n",
    "\n",
    "scores = scores.loc[ scores['class'] != 'mid', ['spkID','gender','class']]\n",
    "\n",
    "scores.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['class'] = pd.Categorical(scores['class'], categories=['low','high'])\n",
    "\n",
    "print(scores.groupby(['gender','class']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split speakers into train (75%) and test (25%) speakers with class and gender balance (stratified) by creating the dummy \"gendertrait\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stratified random partition for train and test\n",
    "\n",
    "scores['genderclass']=scores[['gender', 'class']].apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "indexes = np.arange(0,len(scores))\n",
    "classes = scores['class']\n",
    "train_i, test_i, train_y, test_y = train_test_split(indexes, \n",
    "                                                    classes, \n",
    "                                                    test_size=0.25, \n",
    "                                                    stratify = scores['genderclass'], \n",
    "                                                    random_state=2302)\n",
    "\n",
    "scores_train = scores.iloc[train_i,:] \n",
    "scores_test = scores.iloc[test_i,:] \n",
    "\n",
    "print('Number of speakers in Train:',len(scores_train))\n",
    "print('Number of speakers in Test:',len(scores_test))\n",
    "\n",
    "print('Number of w-high speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='whigh']) )\n",
    "print('Number of m-high speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='mhigh']) )\n",
    "print('Number of w-low speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='wlow']) )\n",
    "print('Number of m-high speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='mlow']) )\n",
    "\n",
    "print('Number of w-high speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='whigh']) )\n",
    "print('Number of m-high speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='mhigh']) )\n",
    "print('Number of w-low speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='wlow']) )\n",
    "print('Number of m-low speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='mlow']) )\n",
    "\n",
    "\n",
    "# # save these data for other evaluations\n",
    "scores_train.iloc[:,0:3].to_csv(r'..\\data\\generated_data\\speakerIDs_cls_WAAT_train.csv', index=False)\n",
    "scores_test.iloc[:,0:3].to_csv(r'..\\data\\generated_data\\speakerIDs_clss_WAAT_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech features\n",
    "\n",
    "Speech features have been extracted from the semi-spontaneous dialogs uttered by the 300 speakers of the [NSC corpus](http://www.qu.tu-berlin.de/?id=nsc-corpus). \n",
    "\n",
    "Each semi-spontaneous dialog was splitted into 3 segments of approx. 20s, and the 88 [eGeMAPS](http://ieeexplore.ieee.org/document/7160715/) speech features were extracted from each segment (see ..\\feature_extraction).\n",
    "\n",
    "299 speakers recorded 4 semi-spontaneous dialogs, and 1 female speaker recorded 1 semi-spontaneous dialog. Total = 1197 dialogs * 3 segments = 3591 speech files.\n",
    "\n",
    "Unfortunately, no subjective ratings have been collected for the spontaneous dialogs d5, d7, or d8. However, we use the speech features in order to have more instances with which to train and test the models.\n",
    "\n",
    "**I assume** that the speakers' trait classes remain constant across recordings, that is, is a speaker is perceived as 'high' in the _intelligent_ trait for dialog 6 (d6, pizza dialog), then this perception would be the same for the other dialogs uttered by the same speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load speech features\n",
    "\n",
    "path = \"https://raw.githubusercontent.com/laufergall/ML_Speaker_Characteristics/master/data/extracted_features/\"\n",
    "\n",
    "url = path + \"/eGeMAPSv01a_semispontaneous_splitted.csv\"\n",
    "s = requests.get(url).content\n",
    "feats =pd.read_csv(io.StringIO(s.decode('utf-8')), sep = ';') # shape: 3591, 89\n",
    "\n",
    "feats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pre-processing features with the transformation **learnt with training data**:\n",
    "\n",
    "* center and scale speech features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate instances according to the train and test partition\n",
    "# instances corresponding to speakers in the mid class will be left out\n",
    "\n",
    "# extract speaker ID from speech file name\n",
    "feats['spkID'] = feats['name'].str.slice(2, 5).astype('int')\n",
    "\n",
    "# appending class label\n",
    "feats_class_train = pd.merge(feats, scores_train[['spkID','genderclass','class']], how='inner')\n",
    "feats_class_test = pd.merge(feats, scores_test[['spkID','genderclass','class']], how='inner')\n",
    "\n",
    "print('Number of high instances in Train:', len(feats_class_train.loc[feats_class_train['class']=='high']) )\n",
    "print('Number of low instances in Train:', len(feats_class_train.loc[feats_class_train['class']=='low']) )\n",
    "print('Number of high instances in Test:', len(feats_class_test.loc[feats_class_test['class']=='high']) )\n",
    "print('Number of low instances in Test:', len(feats_class_test.loc[feats_class_test['class']=='low']) )\n",
    "\n",
    "\n",
    "feats_class_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize speech features  \n",
    "\n",
    "# save feature names\n",
    "feats_names = feats_class_train.drop(['name','spkID','genderclass','class'],axis=1).columns\n",
    "\n",
    "# learn transformation on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats_class_train.drop(['name','spkID','genderclass','class'],axis=1))\n",
    "\n",
    "# numpy n_instances x n_feats\n",
    "feats_s_train = scaler.transform(feats_class_train.drop(['name','spkID','genderclass','class'],axis=1))\n",
    "feats_s_test = scaler.transform(feats_class_test.drop(['name','spkID','genderclass','class'],axis=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model tuning with feature selection\n",
    "\n",
    "Use the train data to find the classifier and its hyperparameters leading to the best performance. Perform feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summarize results of cross-validation on set A for hyperparameter tuning\n",
    "\n",
    "Inputs:\n",
    "- cname: classifier name\n",
    "- grid_result: gridsearch results for this classifier, output of grid.fit(AX, Ay)  \n",
    "- file: open file to write the summary to\n",
    "\"\"\"\n",
    "def summary_tuning(cname, grid_result, file):\n",
    "    \n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    # print best result and append to our lists\n",
    "    print(\"%r -> Best cross-val on A set: %f using %s\" % (cname, grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    # write means, stds, params to file\n",
    "    file.write(\"%r; %r; %r; %r\\n\" % ('model','mean_acc_A', 'stdev_acc_A', 'hyperparameters'))\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        file.write(\"%r; %f; %f; %r\\n\" % (cname, mean, stdev, param))\n",
    "      \n",
    "                           \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Perform nested hyperparameter tuning.\n",
    "Given training data splitted into A, B sets and for each classifier type:\n",
    "Stratified cross-validation for hyperparameter tuning using set A\n",
    "Generates csv file with summary of hp tuning (set A)\n",
    "Evaluate the performance on set B and return accs\n",
    "\n",
    "Input:\n",
    "- AX and BX: features of the train set, splitted\n",
    "- Ay and By: labels of the train set, splitted\n",
    "- get_cls_functions: list of functions tho get classifier and dict of hp to tune\n",
    "\n",
    "Output: \n",
    "pandas dataframe with:\n",
    "- classifiers names\n",
    "- classifiers hyperparameters\n",
    "- accuracies on B set\n",
    "of each tuned classifier corresponding to get_cls_functions\n",
    "\"\"\"    \n",
    "\n",
    "def hp_tuner(AX, BX, Ay, By, get_cls_functions):\n",
    "\n",
    "    # init lists\n",
    "\n",
    "    classifiers_names = []\n",
    "    classifiers = []\n",
    "    hparam_grids = []\n",
    "    grid_results = []\n",
    "    best_accs = [] # on the B set\n",
    "    best_hps = [] # determined with CV on A\n",
    "    trained_cls_list = [] # tuned classifier trained on X,y\n",
    "    \n",
    "    # iterate over list of functions \n",
    "    # to get classifiers and parameters and append to our lists\n",
    "\n",
    "    for fn in get_cls_functions:     \n",
    "        clsname, cls, hp = fn()\n",
    "        classifiers_names.append(clsname)\n",
    "        classifiers.append(cls)\n",
    "        hparam_grids.append(hp)\n",
    "        \n",
    "    # tune hyperparameters witzh GridSearchCV for each classifier\n",
    "    \n",
    "    for i in np.arange(len(classifiers)):\n",
    "\n",
    "        # perform grid search\n",
    "        grid = GridSearchCV(estimator=classifiers[i], \n",
    "                            param_grid=hparam_grids[i], \n",
    "                            n_jobs=1, \n",
    "                            cv=10)\n",
    "        \n",
    "        # This might take a while:\n",
    "        grid_result = grid.fit(AX, Ay) \n",
    "\n",
    "        # append grid search results to our lists\n",
    "        grid_results.append(grid_result)\n",
    "\n",
    "        # summary of hp tuning\n",
    "        # generate one csv file per classifier, in this folder\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        summary_tuning(classifiers_names[i], \n",
    "                       grid_result, \n",
    "                       open('.\\data_while_tuning\\ ' + classifiers_names[i]+'_tuning_'+timestr+'.csv','w'))\n",
    "\n",
    "        # evaluate classifier on set B\n",
    "        test_score = grid_result.best_estimator_.score(BX, By)\n",
    "        print(\"%r -> Score on B set: %f\\n\" % (classifiers_names[i], test_score))\n",
    "        best_accs.append(test_score)\n",
    "        best_hps.append(grid_result.best_params_)\n",
    "        \n",
    "        # train classifier using all training data with this classifier\n",
    "        X = np.concatenate((AX, BX), axis=0)\n",
    "        y = np.concatenate((Ay, By), axis=0)\n",
    "        trained_cls = grid_result.best_estimator_.fit(X,y)\n",
    "        trained_cls_list.append(trained_cls)\n",
    "    \n",
    "    # create the output dataframe    \n",
    "    d = {'classifiers_names': classifiers_names, 'best_accs': best_accs, 'best_hps': best_hps}\n",
    "    cls_acc_hps = pd.DataFrame(data = d) \n",
    "    \n",
    "    return cls_acc_hps, trained_cls_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code snippet to evaluate classification accuracy:\n",
    "    \n",
    "* Choose data (feature and labels) for train X and y and test Xt and yt\n",
    "* Split train data into A and B sets\n",
    "* Hyperparameter tuner using A and B sets data by calling hp_tuner()\n",
    "    * For each classifier type:\n",
    "        * Stratified cross-validation for hyperparameter tuning using set A\n",
    "        * Evaluate the performance on set B\n",
    "* Select classifier based on the best performance on set B and train it using all training data   \n",
    "* Get performance on test set\n",
    "\n",
    "(Nested hyperparameter tuning inspired by [A. Zheng](http://www.oreilly.com/data/free/evaluating-machine-learning-models.csp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data. Features and labels\n",
    "X = feats_s_train\n",
    "y = feats_class_train['class'].cat.codes\n",
    "\n",
    "# test data. Features and labels\n",
    "Xt = feats_s_test\n",
    "yt = feats_class_test['class'].cat.codes\n",
    "\n",
    "# split train data into 80% and 20% subsets - with balance in trait and gender\n",
    "# give subset A to the inner hyperparameter tuner\n",
    "# and hold out subset B for meta-evaluation\n",
    "AX, BX, Ay, By = train_test_split(X, y, test_size=0.20, stratify = feats_class_train['genderclass'], random_state=2302)\n",
    "\n",
    "print('Number of instances in A (hyperparameter tuning):',AX.shape[0])\n",
    "print('Number of instances in B (meta-evaluation):',BX.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe with results from hp tuner to be appended\n",
    "tuning_all = pd.DataFrame()\n",
    "\n",
    "# list with tuned classifiers trained on training data, to be appended\n",
    "trained_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feats_class_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-41a6d8f32ab3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# original features and class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfeats_class_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'.\\data_while_tuning\\feats_class_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mfeats_class_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'.\\data_while_tuning\\feats_class_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feats_class_train' is not defined"
     ]
    }
   ],
   "source": [
    "# save splits\n",
    "\n",
    "import csv\n",
    "\n",
    "# original features and class\n",
    "feats_class_train.to_csv(r'.\\data_while_tuning\\feats_class_train.csv', index=False)\n",
    "feats_class_test.to_csv(r'.\\data_while_tuning\\feats_class_test.csv', index=False)\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "np.save(r'.\\data_while_tuning\\X.npy', X)\n",
    "np.save(r'.\\data_while_tuning\\y.npy', y)\n",
    "np.save(r'.\\data_while_tuning\\Xt.npy', Xt)\n",
    "np.save(r'.\\data_while_tuning\\yt.npy', yt)\n",
    "\n",
    "# # A/B splits, features and labels\n",
    "np.save(r'.\\data_while_tuning\\AX.npy', AX)\n",
    "np.save(r'.\\data_while_tuning\\BX.npy', BX)\n",
    "np.save(r'.\\data_while_tuning\\Ay.npy', Ay)\n",
    "np.save(r'.\\data_while_tuning\\By.npy', By)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving outpus of hp tuning to disk\n",
    "Called after tuning each classifier\n",
    "\n",
    "Input:\n",
    "- tuning_all: pandas df with tuning results\n",
    "- trained_all: list of all classifiers trained on training data\n",
    "\"\"\" \n",
    "def save_tuning(tuning_all, trained_all):\n",
    "    \n",
    "    # save tuning_all\n",
    "    tuning_all.to_csv(r'.\\data_while_tuning\\tuning_all.csv', index=False)\n",
    "    \n",
    "    # save trained_all\n",
    "    for i in np.arange(len(trained_all)):\n",
    "        filename = r'.\\data_while_tuning\\trained_' + tuning_all.loc[i, 'classifiers_names'] + '.sav'\n",
    "        pickle.dump(trained_all[i], open(filename, 'wb'))\n",
    "        \n",
    "\"\"\"\n",
    "Loading outpus of hp tuning from disk\n",
    "Called to recover what was tuned and trained in previous sessions\n",
    "\n",
    "Output:\n",
    "- tuning_all: pandas df with tuning results\n",
    "- trained_all: list of all classifiers trained on training data\n",
    "\"\"\" \n",
    "def load_tuning():\n",
    "    \n",
    "    # load tuning_all\n",
    "    tuning_all = pd.read_csv(r'.\\data_while_tuning\\tuning_all.csv')\n",
    "    \n",
    "    # load trained_all\n",
    "    trained_all=[]\n",
    "    for i in np.arange(len(tuning_all)):\n",
    "        filename = r'.\\data_while_tuning\\trained_' + tuning_all.loc[i, 'classifiers_names'] + '.sav'\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        trained_all.append(loaded_model)\n",
    "        \n",
    "    return tuning_all, trained_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling hp_tuner() for each classifier individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Recover ** when new ipynb session started\n",
    "(Workaround for working with hyperparameter tuning during several days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# original features and class\n",
    "feats_class_train = pd.read_csv(r'.\\data_while_tuning\\feats_class_train.csv')\n",
    "feats_class_test = pd.read_csv(r'.\\data_while_tuning\\feats_class_test.csv')\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "X = np.load(r'.\\data_while_tuning\\X.npy')\n",
    "y = np.load(r'.\\data_while_tuning\\y.npy')\n",
    "Xt = np.load(r'.\\data_while_tuning\\Xt.npy')\n",
    "yt = np.load(r'.\\data_while_tuning\\yt.npy')\n",
    "\n",
    "# # A/B splits, features and labels\n",
    "AX = np.load(r'.\\data_while_tuning\\AX.npy')\n",
    "BX = np.load(r'.\\data_while_tuning\\BX.npy')\n",
    "Ay = np.load(r'.\\data_while_tuning\\Ay.npy')\n",
    "By = np.load(r'.\\data_while_tuning\\By.npy')\n",
    "\n",
    "# Loading outpus of hp tuning from disk\n",
    "tuning_all, trained_all = load_tuning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this after each experiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tuning_all (.csv) and trained_all (nameclassifier.sav)\n",
    "save_tuning(tuning_all, trained_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'GaussianNB' -> Best cross-val on A set: 0.614449 using {}\n",
      "'GaussianNB' -> Score on B set: 0.644377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\"\"\"\n",
    "Naive Bayes Classifier\n",
    "\"\"\"\n",
    "def get_GaussianNB2tune():\n",
    "\n",
    "    model = GaussianNB()\n",
    "    hp = dict()\n",
    "    return 'GaussianNB', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_GaussianNB2tune])\n",
    "\n",
    "# update lists of tuning info and trained classifiers\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'LogisticRegression' -> Best cross-val on A set: 0.761977 using {'penalty': 'l1'}\n",
      "'LogisticRegression' -> Score on B set: 0.747720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\"\"\"\n",
    "Logistic Regression\n",
    "\"\"\"\n",
    "def get_LogisticRegression2tune():\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    hp = dict(\n",
    "        penalty = ['l1','l2']\n",
    "    )\n",
    "    return 'LogisticRegression', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_LogisticRegression2tune])\n",
    "\n",
    "# update lists of tuning info and trained classifiers\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\"\"\"\n",
    "K Nearest Neighbors\n",
    "\"\"\"\n",
    "def get_KNeighborsClassifier2tune():\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "    hp = dict(\n",
    "        n_neighbors = list(range(1,50))\n",
    "    )\n",
    "    return 'KNeighborsClassifier', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_KNeighborsClassifier2tune])\n",
    "\n",
    "# # update lists of tuning info and trained classifiers\n",
    "# tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "# trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\"\"\"\n",
    "Support Vector Machines\n",
    "\"\"\"\n",
    "def get_SVC2tune():\n",
    "    \n",
    "    model = SVC()\n",
    "    hp = dict(\n",
    "        C = np.logspace(-5,3,num=9),\n",
    "        kernel = ['poly'], #['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        degree = [2] #, # only 'poly' kernel\n",
    "        #gamma = np.logspace(-5,3,num=9)\n",
    "    )\n",
    "    return 'SVC', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_SVC2tune])\n",
    "\n",
    "# # update lists of tuning info and trained classifiers\n",
    "# tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "# trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\"\"\"\n",
    "Decision Trees\n",
    "\"\"\"\n",
    "def get_DecisionTreeClassifier2tune():\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    hp = dict(\n",
    "        max_depth = np.arange(2,4)#np.arange(2,11)\n",
    "    )\n",
    "    return 'DecisionTreeClassifier', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_DecisionTreeClassifier2tune])\n",
    "\n",
    "# # update lists of tuning info and trained classifiers\n",
    "# tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "# trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "def get_RandomForestClassifier2tune():\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    hp = dict(\n",
    "        n_estimators = np.arange(2,4)#np.arange(2,51)\n",
    "    )\n",
    "    return 'RandomForestClassifier', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_RandomForestClassifier2tune])\n",
    "\n",
    "# # update lists of tuning info and trained classifiers\n",
    "# tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "# trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected classifier based on the best performance on B: 'LogisticRegression' (accB = 0.75)\n"
     ]
    }
   ],
   "source": [
    "# select the classifier that gave the maximum acc on B set\n",
    "best_accs = tuning_all['best_accs']\n",
    "i_best = best_accs.idxmax()\n",
    "\n",
    "print('Selected classifier based on the best performance on B: %r (accB = %0.2f)' % (tuning_all.loc[i_best,'classifiers_names'], round(best_accs[i_best],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_on_test: 0.713768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.71      0.67       228\n",
      "          1       0.78      0.72      0.75       324\n",
      "\n",
      "avg / total       0.72      0.71      0.72       552\n",
      "\n",
      "[[162  66]\n",
      " [ 92 232]]\n"
     ]
    }
   ],
   "source": [
    "# predictions on the test set\n",
    "yt_pred = trained_all[i_best][0].predict(Xt)\n",
    "\n",
    "score_on_test = trained_all[i_best][0].score(Xt,yt)\n",
    "\n",
    "print(\"score_on_test: %f\" % score_on_test) \n",
    "\n",
    "cm = confusion_matrix(yt, yt_pred)\n",
    "print(classification_report(yt, yt_pred, digits = 2))\n",
    "print(cm)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
