---
title: "Mc Nemar's test of statistical differences between voice degradations"
author: "Laura Fernández Gallardo"
date: "April 2018"
output: 
  github_document:
  toc: true
toc_depth: 3
---
  


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root.dir = './')
knitr::opts_chunk$set(fig.width=10, fig.height=5, dpi=150)
```


```{r echo=FALSE}

# clear
rm(list=ls())

```

```{r message=FALSE, warning=FALSE}

# Libraries needed:

library(knitr) # for kable


``` 

## Objectives

To assess the effects of speech degradations on the perfromance of Random Forest classification and Support Vector Machine classification of WAAT.

Classification was performed in [this notebook](https://github.com/laufergall/ML_Speaker_Characteristics/blob/master/classification/04_classification_degraded_speech.ipynb), from where true class 'yt_spk' classification predictions 'yt_spk_pred' were genderated for each classifier.

Binomial tests to test for statistically significant differences in classification accuracy across different channel degradations.

[1] Noirhomme, Q., Lesenfants, D., Gomez, F., Soddu, A., Schrouff, J., Garraux, G., Luxen, A., Phillips, C., Laureys, S. "Biased binomial assessment of cross-validated estimation of classification accuracies illustrated in diagnosis predictions," NeuroImage: Clinical 4, 687-694, 2014.
 
Variables:

```{r}

siglevel = 0.01 # p-value

```

## Load predictions for each classifier

NA predictions generated when transmission simulations were unsuccessfull (around 7%).

```{r echo=FALSE} 

# setting paths and loading data
setwd("D:/Users/fernandez.laura/Documents/Work/Projects_Github/ML_Speaker_Characteristics/classification")

# read true classes

classes <- read.csv('../data/generated_data/speakerIDs_cls_WAAT_all.csv') 


# read predictions: read csv files which contain 'cls_test_distortionsdegradations'

res <- data.frame()
file.names <- dir('./performance_channels', pattern ="cls_test_distortionsdegradations") 
for(i in 1:length(file.names)){
  dataread <- read.csv(paste0('./performance_channels/',file.names[i]), header=TRUE, sep=",") 
  # get classifier name
  pos <- gregexpr('_', file.names[i])
  dataread$cls <- substr(file.names[i], pos[[1]][3]+1, pos[[1]][4]-1)
  res <- rbind(res,dataread)
}

# add 'NB' or 'WB' to Speex codec according to bandwidth
res$codec <- as.character(res$codec)
speex_codec <-  res[res$codec == 'Speex', names(res)=='codec']
speex_bw <-  res[res$codec == 'Speex', names(res)=='bandwidth']
res[res$codec == 'Speex',names(res)=='codec'] <- paste0(speex_codec,'-',speex_bw)

```

Speakers:

```{r}

n_spk <- nrow(classes)
summary(classes)

```

Channel distortions:

```{r}

levels(res$distortion)

```

Channel degradations:

```{r}

levels(res$degradation)

```

Classifiers:

```{r}

unique(res$cls)

```
 
## Load true classes

Load true classes of only high/low WAAT: 183 speakers

```{r}

classes = read.csv('../data/generated_data/speakerIDs_cls_WAAT_all.csv') 

# predictions as 'high' class coded as 0 and 'low' class coded as 1
classes$class_num <- as.numeric(as.factor(classes$class))-1

# sort according to spkID, since predictions are sorted
classes <- classes[order(classes$spkID),]


```


 
## Binomial tests 

Run Binomial Test for each pair of codecs. 

The null hypothesis is that the probability of success (accuracy - not per-class averaged) is the same for the classification with two different speech degradations.

As stated in [1]: "With a limited number of trials, the results of a classifier are seen as the results of tossing a coin, an unfair coin, which can be modeled as a Bernoulli trial".

### Performance over chance level

For each classifier: 
* Perform Binomial Test over chance level (0.5) for each degradation
* Considering constant packet loss rate = 0 and jitter = 0


```{r}

tests_all <- data.frame()

# for each classifier
for (cl in unique(res$cls)){
  
  # subset for classifier and for distortion condition
  res_cls <- res[res$cls == cl,]
  res_cls <- res_cls[res_cls$distortion == 'pl00ji00',]
  
  # tests for this cls
  tests_cls <- data.frame(res_cls$degradation, pval_greater=0, cls=cl)
  
  # for each degradation, perform Binomial Test
  for (i in 1:nrow(tests_cls)){

    # predictions of each degradation
    preds <- as.matrix(res_cls[res_cls$degradation==toString(res_cls$degradation[i]),c(5:(5+n_spk-1))])

    # number of successes 
    n_succ <- sum(preds==classes$class_num)
    
    
    # binom.test and store p.value
    bt <- binom.test(n_succ, n_spk, 0.5, alternative="greater")
    tests_cls$pval_greater[i] <- bt$p.value 
    
  }
  
  tests_all <- rbind(tests_all, tests_cls)
}
  

```

Looking only at significant differences at 'siglevel' level found in RF and SVC classification.

```{r}

# subset for classifiers 
tests_RF_SVC <- tests_all[tests_all$cls %in% c('RandomForestClassifier','SVCrbf'),]
 
# subset for significant differences 
tests_RF_SVC_sig_greater <- tests_RF_SVC[tests_RF_SVC$pval_greater < siglevel,]

```

Performance significantly over chance level:

For RF classification: Clean, WB_AMRWB+ (all bitrates), and SWB_Opus (at 32 and 64 kbit/s).

For SVC classification: Clean, WB_AMRWB (all bitrates except for 23.85 kbit/s), and WB_AMRWB+ (all bitrates).


### Each pair of degradations

For each classifier: 
* Perform Binomial Test with each pair of degradations
* Considering constant packet loss rate = 0 and jitter = 0

Generating combinations of conditions to compare:
For each codec, choose the bitrate offering the highest performance. In case of ties, choose the lowest bitrate.


```{r}

combs_all <- data.frame()

# for each classifier
for (cl in unique(res$cls)){
  
  # subset for classifier and for distortion condition
  res_cls <- res[res$cls == cl,]
  res_cls <- res_cls[res_cls$distortion == 'pl00ji00',]
  
  # generate combinations
  chosen <- c()
  for (co in unique(res_cls$codec)){
    res_cls_co <- res_cls[res_cls$codec == co,]
    posmax <- which.max(res_cls_co$average.per.class.accuracy)
    chosen <- append(chosen, toString(res_cls_co$degradation[posmax]))
  }
  
  combs_cls <- data.frame(t(combn(chosen, 2)), pval_greater=0, pval_less=0, cls=cl)
  
  # for each pair of degradations, perform Binomial Test
  for (i in 1:nrow(combs_cls )){
    pair <- combs_cls[i,]
    
    # predictions of each degradation
    preds1 <- as.matrix(res_cls[res_cls$degradation==toString(pair[1,1]),c(5:(5+n_spk-1))])
    preds2 <- as.matrix(res_cls[res_cls$degradation==toString(pair[1,2]),c(5:(5+n_spk-1))])

    # number of successes for preds1 and preds2
    n_succ1 <- sum(preds1==classes$class_num)
    n_succ2 <- sum(preds2==classes$class_num)
    
    
    # binom.test for "greater" and store p.value
    bt <- binom.test(n_succ1, n_spk, n_succ2/n_spk, alternative="greater")
    combs_cls$pval_greater[i] <- bt$p.value 
    
    # binom.test for "less" and store p.value
    bt <- binom.test(n_succ1, n_spk, n_succ2/n_spk, alternative="less")
    combs_cls$pval_less[i] <- bt$p.value 
    
  }
  
  combs_all <- rbind(combs_all, combs_cls)
}
  

```


Looking only at significant differences at 'siglevel' level found in RF and SVC classification.

```{r}

# subset for classifiers 
combs_RF_SVC <- combs_all[combs_all$cls %in% c('RandomForestClassifier','SVCrbf'),]
 
# subset for significant differences 
combs_RF_SVC_sig_greater <- combs_RF_SVC[combs_RF_SVC$pval_greater < siglevel,]

combs_RF_SVC_sig_less <- combs_RF_SVC[combs_RF_SVC$pval_less < siglevel,]


```



```{r}
print('Codec on the left offers significantly higher performance than the codec on the right')

print('Significant differences for RF classification:')
cbind( as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='RandomForestClassifier',2]), as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='RandomForestClassifier',1]))
cbind( as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='RandomForestClassifier',2]), as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='RandomForestClassifier',1]))
print(' ')
print('Significant differences for SVC classification:')
cbind( as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='SVCrbf',2]), as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='SVCrbf',1]))
cbind( as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='SVCrbf',2]), as.character(combs_RF_SVC_sig_less[combs_RF_SVC_sig_less$cls=='SVCrbf',1]))
```




## Conclusions

It can therefore be concluded that:


For both classifiers:

* Performance significantly over chance level found for Clean and WB_AMRWB+ (all bitrates).

* Accuracy with clean speech statistically significantly higher than that with all NB-, WB-, and SWB- coded speech, except for AMR-WB+.


For RF classification:

* The AMRWB+ codec offers statistically significantly superior accuracy than the rest of codecs in all bandwidths.

* No difference between the rest of WB codecs and NB codecs, except for AMRWB, which offers higher performance than G711.

* The three SWB codecs also improve the performance of G.711 significantly.

* In addition, SWB_Opus provides significantly higher performance than GSMEFR_12_2, Speex-NB, and Speex-WB.  


For SVC classification:

* The AMRWB+ codec offers statistically significantly superior accuracy than the rest of codecs in all bandwidths, except for AMR-WB - no statistical difference in accuracy between AMRWB+ and AMRWB.

* The AMRWB+ codec offers statistically significantly superior accuracy than the rest of codecs in all bandwidths, except for SWB_G7221C - no statistical difference in accuracy between AMRWB and SWB_G7221C.

* All WB and SWB codecs offer statistically significantly superior accuracy than the NB codecs except for G.722 and Opus - no statistical difference in accuracy between G.722 (WB) and NB codecs and between Opus (SWB) and AMRNB.






